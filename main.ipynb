{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon ML Challenge 2025: Product Pricing Solution\n",
    "\n",
    "This notebook implements the winning solution strategy using LightGBM and custom Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAccuracyFeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.tfidf = None\n",
    "        self.scaler = RobustScaler()  # IMPROVEMENT: Better for outliers\n",
    "        self.model = None\n",
    "        self.feature_columns = []\n",
    "        self.unit_categories = None\n",
    "        self.numeric_medians = {}\n",
    "        self.categorical_columns = None\n",
    "        self.expected_feature_count = None\n",
    "        self.brand_categories = None\n",
    "        self.category_categories = None\n",
    "        \n",
    "    def extract_brand(self, text):\n",
    "        \"\"\"IMPROVED: More comprehensive brand detection\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 'unknown'\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Expanded brand mapping\n",
    "        brands = {\n",
    "            'amazon': ['amazon basics', 'amazon', 'amazonbasics'],\n",
    "            'samsung': ['samsung'],\n",
    "            'apple': ['apple', 'iphone', 'ipad', 'macbook', 'airpods'],\n",
    "            'sony': ['sony', 'playstation'],\n",
    "            'lg': ['lg'],\n",
    "            'intel': ['intel', 'core i3', 'core i5', 'core i7', 'core i9'],\n",
    "            'nvidia': ['nvidia', 'geforce', 'rtx', 'gtx'],\n",
    "            'amd': ['amd', 'ryzen', 'radeon'],\n",
    "            'corsair': ['corsair'],\n",
    "            'logitech': ['logitech'],\n",
    "            'razer': ['razer'],\n",
    "            'dell': ['dell', 'alienware'],\n",
    "            'hp': ['hp', 'hewlett'],\n",
    "            'lenovo': ['lenovo', 'thinkpad'],\n",
    "            'canon': ['canon', 'eos'],\n",
    "            'nikon': ['nikon'],\n",
    "            'gopro': ['gopro'],\n",
    "            'dji': ['dji'],\n",
    "            'bose': ['bose'],\n",
    "            'jbl': ['jbl'],\n",
    "            'beats': ['beats'],\n",
    "            'anker': ['anker'],\n",
    "            'belkin': ['belkin'],\n",
    "            'netgear': ['netgear'],\n",
    "            'tp-link': ['tp-link', 'tplink'],\n",
    "            'asus': ['asus', 'rog'],\n",
    "            'microsoft': ['microsoft', 'surface', 'xbox'],\n",
    "            'panasonic': ['panasonic'],\n",
    "            'philips': ['philips'],\n",
    "            'sandisk': ['sandisk'],\n",
    "            'western digital': ['western digital', 'wd'],\n",
    "            'seagate': ['seagate'],\n",
    "            'kingston': ['kingston'],\n",
    "            'crucial': ['crucial']\n",
    "        }\n",
    "        \n",
    "        for brand, keywords in brands.items():\n",
    "            if any(kw in text_lower for kw in keywords):\n",
    "                return brand\n",
    "        \n",
    "        # Extract first capitalized word\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if len(word) > 2 and word[0].isupper():\n",
    "                return word.lower()\n",
    "        \n",
    "        return 'generic'\n",
    "    \n",
    "    def extract_product_category(self, text):\n",
    "        \"\"\"IMPROVED: More granular categories\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 'other'\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        categories = {\n",
    "            'laptop': ['laptop', 'notebook', 'macbook', 'chromebook'],\n",
    "            'phone': ['phone', 'smartphone', 'iphone', 'galaxy'],\n",
    "            'tablet': ['tablet', 'ipad'],\n",
    "            'computer': ['desktop', 'pc', 'tower', 'workstation'],\n",
    "            'cpu': ['cpu', 'processor', 'ryzen', 'core i'],\n",
    "            'gpu': ['gpu', 'graphics card', 'geforce', 'radeon'],\n",
    "            'monitor': ['monitor', 'display', 'screen'],\n",
    "            'keyboard': ['keyboard'],\n",
    "            'mouse': ['mouse', 'mice'],\n",
    "            'headphone': ['headphone', 'headset', 'earbuds', 'airpods'],\n",
    "            'speaker': ['speaker', 'soundbar'],\n",
    "            'networking': ['router', 'modem', 'ethernet', 'wifi', 'switch', 'network'],\n",
    "            'storage': ['ssd', 'hard drive', 'hdd', 'usb', 'sd card', 'flash'],\n",
    "            'ram': ['ram', 'memory', 'ddr4', 'ddr5'],\n",
    "            'motherboard': ['motherboard', 'mobo'],\n",
    "            'psu': ['power supply', 'psu'],\n",
    "            'case': ['case', 'chassis', 'tower case'],\n",
    "            'cooling': ['cooler', 'cooling', 'fan', 'radiator'],\n",
    "            'cable': ['cable', 'cord', 'wire', 'hdmi', 'usb-c'],\n",
    "            'charger': ['charger', 'adapter', 'power adapter'],\n",
    "            'camera': ['camera', 'webcam', 'dslr', 'mirrorless'],\n",
    "            'components': ['resistor', 'capacitor', 'transistor', 'diode', 'ic', 'led'],\n",
    "            'food': ['food', 'drink', 'sauce', 'snack', 'cookie', 'butter', 'protein'],\n",
    "            'home': ['furniture', 'decoration', 'lamp', 'curtain', 'bedding'],\n",
    "            'tools': ['tool', 'drill', 'saw', 'hammer', 'wrench'],\n",
    "            'sports': ['sports', 'bike', 'yoga', 'fitness', 'gym'],\n",
    "            'beauty': ['beauty', 'shampoo', 'makeup', 'cosmetics'],\n",
    "            'cooking': ['pot', 'pan', 'knife', 'blender', 'cookware']\n",
    "        }\n",
    "        \n",
    "        for category, keywords in categories.items():\n",
    "            if any(kw in text_lower for kw in keywords):\n",
    "                return category\n",
    "        \n",
    "        return 'other'\n",
    "    \n",
    "    def extract_numbers(self, text):\n",
    "        \"\"\"NEW: Extract all numeric values for analysis\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', str(text))\n",
    "        return [float(n) for n in numbers if float(n) > 0]\n",
    "    \n",
    "    def extract_storage_capacity(self, text):\n",
    "        \"\"\"NEW: Specifically extract storage (GB, TB)\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Look for TB\n",
    "        tb_match = re.search(r'(\\d+)\\s*tb', text_lower)\n",
    "        if tb_match:\n",
    "            return float(tb_match.group(1)) * 1024  # Convert to GB\n",
    "        \n",
    "        # Look for GB\n",
    "        gb_match = re.search(r'(\\d+)\\s*gb', text_lower)\n",
    "        if gb_match:\n",
    "            return float(gb_match.group(1))\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def extract_ram_capacity(self, text):\n",
    "        \"\"\"NEW: Extract RAM capacity\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Look for RAM mentions\n",
    "        ram_patterns = [\n",
    "            r'(\\d+)\\s*gb\\s*ram',\n",
    "            r'ram[:\\s]+(\\d+)\\s*gb',\n",
    "            r'(\\d+)gb\\s*ddr'\n",
    "        ]\n",
    "        \n",
    "        for pattern in ram_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                return float(match.group(1))\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def extract_wattage(self, text):\n",
    "        \"\"\"NEW: Extract power/wattage\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        watt_match = re.search(r'(\\d+)\\s*w(?:att)?', text_lower)\n",
    "        if watt_match:\n",
    "            return float(watt_match.group(1))\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def extract_screen_size(self, text):\n",
    "        \"\"\"NEW: Extract screen size in inches\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        inch_patterns = [\n",
    "            r'(\\d+\\.?\\d*)\\s*inch',\n",
    "            r'(\\d+\\.?\\d*)\"',\n",
    "            r'(\\d+\\.?\\d*)″'\n",
    "        ]\n",
    "        \n",
    "        for pattern in inch_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                size = float(match.group(1))\n",
    "                if 5 <= size <= 100:  # Reasonable screen sizes\n",
    "                    return size\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def extract_weight(self, text):\n",
    "        \"\"\"NEW: Extract weight\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Pounds\n",
    "        lb_match = re.search(r'(\\d+\\.?\\d*)\\s*(?:lb|pound)', text_lower)\n",
    "        if lb_match:\n",
    "            return float(lb_match.group(1)) * 0.453592  # Convert to kg\n",
    "        \n",
    "        # Kg\n",
    "        kg_match = re.search(r'(\\d+\\.?\\d*)\\s*kg', text_lower)\n",
    "        if kg_match:\n",
    "            return float(kg_match.group(1))\n",
    "        \n",
    "        # Ounces\n",
    "        oz_match = re.search(r'(\\d+\\.?\\d*)\\s*oz', text_lower)\n",
    "        if oz_match:\n",
    "            return float(oz_match.group(1)) * 0.0283495  # Convert to kg\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def extract_quality_tier(self, text):\n",
    "        \"\"\"Enhanced quality detection\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 'standard'\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        premium_keywords = ['premium', 'luxury', 'pro', 'professional', 'elite', \n",
    "                          'high-end', 'limited edition', 'exclusive', 'titanium', \n",
    "                          'diamond', 'ultra', 'max', 'plus', 'advanced']\n",
    "        if any(kw in text_lower for kw in premium_keywords):\n",
    "            return 'premium'\n",
    "        \n",
    "        budget_keywords = ['budget', 'basic', 'entry-level', 'starter', 'economy', \n",
    "                         'value', 'cheap', 'inexpensive', 'affordable', 'lite']\n",
    "        if any(kw in text_lower for kw in budget_keywords):\n",
    "            return 'budget'\n",
    "        \n",
    "        return 'standard'\n",
    "    \n",
    "    def extract_pack_quantity(self, text):\n",
    "        \"\"\"Enhanced pack detection\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 1, 'single'\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Pack of X\n",
    "        pack_match = re.search(r'pack of (\\d+)', text_lower)\n",
    "        if pack_match:\n",
    "            qty = int(pack_match.group(1))\n",
    "            return qty, f'pack_{min(qty, 10)}'\n",
    "        \n",
    "        # X pack\n",
    "        pack_match2 = re.search(r'(\\d+)[\\s-]pack', text_lower)\n",
    "        if pack_match2:\n",
    "            qty = int(pack_match2.group(1))\n",
    "            return qty, f'pack_{min(qty, 10)}'\n",
    "        \n",
    "        # X piece/set/count\n",
    "        qty_match = re.search(r'(\\d+)\\s*(?:piece|set|box|count)', text_lower)\n",
    "        if qty_match:\n",
    "            qty = int(qty_match.group(1))\n",
    "            if qty > 1:\n",
    "                return qty, f'multi_{min(qty, 10)}'\n",
    "        \n",
    "        return 1, 'single'\n",
    "    \n",
    "    def count_specifications(self, text):\n",
    "        \"\"\"Enhanced spec counting\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        spec_keywords = [\n",
    "            'voltage', 'current', 'watt', 'hz', 'mhz', 'ghz', 'rpm', \n",
    "            'resolution', 'fps', 'dpi', 'ppi',\n",
    "            'gb', 'mb', 'kb', 'tb',\n",
    "            'inch', 'cm', 'mm', 'meter',\n",
    "            'kg', 'lb', 'oz', 'gram',\n",
    "            'liter', 'ml', 'gallon',\n",
    "            'temperature', 'celsius', 'fahrenheit',\n",
    "            'pressure', 'psi', 'bar',\n",
    "            'speed', 'bandwidth', 'latency',\n",
    "            'capacity', 'output', 'input',\n",
    "            'compatibility', 'wireless', 'bluetooth'\n",
    "        ]\n",
    "        \n",
    "        count = sum(1 for keyword in spec_keywords if keyword in text_lower)\n",
    "        return count\n",
    "    \n",
    "    def has_warranty_mention(self, text):\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        warranty_keywords = ['warranty', 'guarantee', 'guaranteed', 'year warranty', \n",
    "                           'lifetime warranty', 'coverage', 'protected']\n",
    "        return 1 if any(kw in text_lower for kw in warranty_keywords) else 0\n",
    "    \n",
    "    def has_discount_mention(self, text):\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        discount_keywords = ['discount', 'sale', 'promo', 'limited time', \n",
    "                           'save', 'off', '%', 'deal', 'clearance', 'reduced']\n",
    "        return 1 if any(kw in text_lower for kw in discount_keywords) else 0\n",
    "    \n",
    "    def has_color_mention(self, text):\n",
    "        \"\"\"NEW: Check for color mentions\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        text_lower = str(text).lower()\n",
    "        colors = ['black', 'white', 'red', 'blue', 'green', 'silver', 'gold', \n",
    "                 'gray', 'pink', 'purple', 'yellow', 'orange', 'brown']\n",
    "        return 1 if any(color in text_lower for color in colors) else 0\n",
    "    \n",
    "    def has_model_number(self, text):\n",
    "        \"\"\"NEW: Check for model numbers (often premium products)\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        # Pattern: Letters followed by numbers (e.g., GTX1080, i7-9700K)\n",
    "        pattern = r'[A-Z]{2,}\\d+|[A-Z]\\d+-\\d+'\n",
    "        return 1 if re.search(pattern, str(text)) else 0\n",
    "    \n",
    "    def extract_year(self, text):\n",
    "        \"\"\"NEW: Extract year (newer = higher price)\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        \n",
    "        # Look for years 2015-2025\n",
    "        year_match = re.search(r'20(1[5-9]|2[0-5])', str(text))\n",
    "        if year_match:\n",
    "            return int(year_match.group(0))\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def safe_one_hot_encode(self, series, prefix, expected_categories):\n",
    "        \"\"\"Consistent one-hot encoding\"\"\"\n",
    "        dummies = pd.get_dummies(series, prefix=prefix)\n",
    "        \n",
    "        for category in expected_categories:\n",
    "            col_name = f\"{prefix}_{category}\"\n",
    "            if col_name not in dummies.columns:\n",
    "                dummies[col_name] = 0\n",
    "        \n",
    "        expected_columns = [f\"{prefix}_{cat}\" for cat in expected_categories]\n",
    "        dummies = dummies.reindex(columns=expected_columns, fill_value=0)\n",
    "        \n",
    "        return dummies.astype(np.float32)\n",
    "    \n",
    "    def extract_advanced_features(self, df, is_training=True):\n",
    "        \"\"\"ENHANCED: More features for better accuracy\"\"\"\n",
    "        print(\"Extracting advanced features...\")\n",
    "        \n",
    "        # Basic value/unit\n",
    "        df['Value'] = pd.to_numeric(\n",
    "            df['catalog_content'].str.extract(r'Value:\\s*([\\d\\.]+)')[0], \n",
    "            errors='coerce'\n",
    "        )\n",
    "        df['Unit'] = df['catalog_content'].str.extract(r'Unit:\\s*(\\w+)')[0]\n",
    "        df['Unit'] = df['Unit'].fillna('missing')\n",
    "        \n",
    "        if is_training:\n",
    "            self.unit_categories = df['Unit'].unique()\n",
    "        \n",
    "        # Categorical features\n",
    "        print(\"  Extracting brands...\")\n",
    "        df['brand'] = df['catalog_content'].apply(self.extract_brand)\n",
    "        if is_training:\n",
    "            self.brand_categories = df['brand'].unique()\n",
    "        \n",
    "        print(\"  Extracting categories...\")\n",
    "        df['category'] = df['catalog_content'].apply(self.extract_product_category)\n",
    "        if is_training:\n",
    "            self.category_categories = df['category'].unique()\n",
    "        \n",
    "        print(\"  Extracting quality tiers...\")\n",
    "        df['quality_tier'] = df['catalog_content'].apply(self.extract_quality_tier)\n",
    "        \n",
    "        print(\"  Extracting pack quantities...\")\n",
    "        pack_info = df['catalog_content'].apply(self.extract_pack_quantity)\n",
    "        df['pack_quantity'] = pack_info.apply(lambda x: x[0])\n",
    "        df['pack_type'] = pack_info.apply(lambda x: x[1])\n",
    "        \n",
    "        # NEW FEATURES\n",
    "        print(\"  Extracting technical specs...\")\n",
    "        df['storage_gb'] = df['catalog_content'].apply(self.extract_storage_capacity)\n",
    "        df['ram_gb'] = df['catalog_content'].apply(self.extract_ram_capacity)\n",
    "        df['wattage'] = df['catalog_content'].apply(self.extract_wattage)\n",
    "        df['screen_size'] = df['catalog_content'].apply(self.extract_screen_size)\n",
    "        df['weight_kg'] = df['catalog_content'].apply(self.extract_weight)\n",
    "        df['year'] = df['catalog_content'].apply(self.extract_year)\n",
    "        \n",
    "        # NEW FLAGS\n",
    "        df['spec_count'] = df['catalog_content'].apply(self.count_specifications)\n",
    "        df['has_warranty'] = df['catalog_content'].apply(self.has_warranty_mention)\n",
    "        df['has_discount'] = df['catalog_content'].apply(self.has_discount_mention)\n",
    "        df['has_color'] = df['catalog_content'].apply(self.has_color_mention)\n",
    "        df['has_model_number'] = df['catalog_content'].apply(self.has_model_number)\n",
    "        \n",
    "        # Text statistics\n",
    "        df['content_length'] = df['catalog_content'].str.len().fillna(0)\n",
    "        df['word_count'] = df['catalog_content'].str.split().str.len().fillna(0)\n",
    "        df['char_density'] = df['content_length'] / (df['word_count'].replace(0, 1))\n",
    "        df['numeric_count'] = df['catalog_content'].str.count(r'\\d+\\.?\\d*')\n",
    "        df['has_decimal'] = df['catalog_content'].str.contains(r'\\d+\\.\\d+').astype(np.int8)\n",
    "        df['bullet_points'] = df['catalog_content'].str.count('Bullet Point')\n",
    "        df['has_description'] = df['catalog_content'].str.contains('Product Description:', case=False).astype(np.int8)\n",
    "        df['uppercase_ratio'] = df['catalog_content'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1))\n",
    "        \n",
    "        # Value-based features\n",
    "        df['has_value_flag'] = (~df['Value'].isna()).astype(np.int8)\n",
    "        df['value_per_word'] = df['Value'] / (df['word_count'].replace(0, 1))\n",
    "        \n",
    "        # NEW: Interaction features (powerful!)\n",
    "        df['storage_x_ram'] = df['storage_gb'] * df['ram_gb']\n",
    "        df['brand_is_premium'] = df['brand'].isin(['apple', 'samsung', 'sony', 'bose', 'dell']).astype(np.int8)\n",
    "        df['is_electronics'] = df['category'].isin(['laptop', 'phone', 'tablet', 'computer', 'gpu', 'cpu']).astype(np.int8)\n",
    "        df['premium_electronics'] = df['brand_is_premium'] * df['is_electronics']\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_cols = ['Value', 'storage_gb', 'ram_gb', 'wattage', 'screen_size', 'weight_kg']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                if is_training:\n",
    "                    median_val = df[col].median()\n",
    "                    self.numeric_medians[col] = median_val\n",
    "                else:\n",
    "                    median_val = self.numeric_medians.get(col, 0)\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "        \n",
    "        # Log transforms for skewed features (IMPROVEMENT)\n",
    "        for col in ['Value', 'storage_gb', 'ram_gb', 'wattage']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_log'] = np.log1p(df[col])\n",
    "        \n",
    "        # Type conversion\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if col in ['content_length', 'word_count', 'numeric_count', 'bullet_points', 'spec_count', 'pack_quantity', 'year']:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_tfidf_features(self, texts):\n",
    "        \"\"\"IMPROVED: Better TF-IDF parameters\"\"\"\n",
    "        if self.tfidf is None:\n",
    "            self.tfidf = TfidfVectorizer(\n",
    "                max_features=10000,  # INCREASED from 8000\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 3),  # IMPROVED: Added trigrams\n",
    "                min_df=3,  # IMPROVED: Lower threshold\n",
    "                max_df=0.90,  # IMPROVED: Higher threshold\n",
    "                sublinear_tf=True,\n",
    "                dtype=np.float32,\n",
    "                norm='l2',\n",
    "                use_idf=True\n",
    "            )\n",
    "            return self.tfidf.fit_transform(texts)\n",
    "        else:\n",
    "            return self.tfidf.transform(texts)\n",
    "\n",
    "    def prepare_full_features(self, df, is_training=True):\n",
    "        \"\"\"ENHANCED: More features\"\"\"\n",
    "        df['catalog_content'] = df['catalog_content'].fillna('missing_content')\n",
    "        df = self.extract_advanced_features(df, is_training=is_training)\n",
    "        \n",
    "        # EXPANDED feature list\n",
    "        self.feature_columns = [\n",
    "            'Value', 'Value_log', 'content_length', 'word_count', 'char_density', \n",
    "            'numeric_count', 'has_decimal', 'has_value_flag', 'value_per_word',\n",
    "            'bullet_points', 'has_description', 'spec_count', 'pack_quantity',\n",
    "            'has_warranty', 'has_discount', 'has_color', 'has_model_number',\n",
    "            'storage_gb', 'storage_gb_log', 'ram_gb', 'ram_gb_log', \n",
    "            'wattage', 'wattage_log', 'screen_size', 'weight_kg',\n",
    "            'year', 'uppercase_ratio', 'storage_x_ram',\n",
    "            'brand_is_premium', 'is_electronics', 'premium_electronics'\n",
    "        ]\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0.0\n",
    "        \n",
    "        # Categorical encoding\n",
    "        categorical_features = {\n",
    "            'brand': self.brand_categories if hasattr(self, 'brand_categories') else df['brand'].unique(),\n",
    "            'category': self.category_categories if hasattr(self, 'category_categories') else df['category'].unique(),\n",
    "            'quality_tier': ['premium', 'standard', 'budget'],\n",
    "            'pack_type': ['single'] + [f'multi_{i}' for i in range(2, 11)] + [f'pack_{i}' for i in range(2, 11)]\n",
    "        }\n",
    "        \n",
    "        categorical_encoded_list = []\n",
    "        for col, categories in categorical_features.items():\n",
    "            if col in df.columns:\n",
    "                encoded = self.safe_one_hot_encode(df[col], col, categories)\n",
    "                categorical_encoded_list.append(encoded)\n",
    "        \n",
    "        if categorical_encoded_list:\n",
    "            categorical_encoded = pd.concat(categorical_encoded_list, axis=1)\n",
    "        else:\n",
    "            categorical_encoded = pd.DataFrame()\n",
    "        \n",
    "        if is_training:\n",
    "            self.categorical_columns = categorical_encoded.columns.tolist()\n",
    "        \n",
    "        # Unit encoding\n",
    "        if self.unit_categories is None:\n",
    "            self.unit_categories = df['Unit'].unique()\n",
    "        \n",
    "        unit_encoded = self.safe_one_hot_encode(df['Unit'], 'Unit', self.unit_categories)\n",
    "        \n",
    "        # TF-IDF\n",
    "        X_tfidf = self.create_tfidf_features(df['catalog_content'])\n",
    "        \n",
    "        # Numeric features with robust scaling\n",
    "        numeric_features = df[self.feature_columns].values.astype(np.float32)\n",
    "        if is_training:\n",
    "            numeric_features = self.scaler.fit_transform(numeric_features)\n",
    "        else:\n",
    "            numeric_features = self.scaler.transform(numeric_features)\n",
    "        \n",
    "        # Combine\n",
    "        cat_array = categorical_encoded.values.astype(np.float32) if not categorical_encoded.empty else np.zeros((len(df), 0))\n",
    "        \n",
    "        X_combined = hstack([\n",
    "            X_tfidf,\n",
    "            csr_matrix(numeric_features),\n",
    "            csr_matrix(unit_encoded.values),\n",
    "            csr_matrix(cat_array)\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "        if is_training:\n",
    "            self.expected_feature_count = X_combined.shape[1]\n",
    "            print(f\"✓ Total features: {self.expected_feature_count}\")\n",
    "        \n",
    "        return X_combined, df\n",
    "\n",
    "    def align_features(self, X_test):\n",
    "        \"\"\"Ensure feature dimensions match\"\"\"\n",
    "        if X_test.shape[1] < self.expected_feature_count:\n",
    "            missing = self.expected_feature_count - X_test.shape[1]\n",
    "            padding = csr_matrix((X_test.shape[0], missing), dtype=np.float32)\n",
    "            X_test = hstack([X_test, padding])\n",
    "        elif X_test.shape[1] > self.expected_feature_count:\n",
    "            X_test = X_test[:, :self.expected_feature_count]\n",
    "        \n",
    "        return X_test\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"OPTIMIZED: Better hyperparameters + K-Fold CV\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING MODEL (MAXIMUM ACCURACY)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # IMPROVEMENT: Use K-Fold for robustness\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        cv_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "            print(f\"\\nFold {fold}/5\")\n",
    "            \n",
    "            X_tr = X_train[train_idx]\n",
    "            y_tr = y_train[train_idx]\n",
    "            X_val = X_train[val_idx]\n",
    "            y_val = y_train[val_idx]\n",
    "            \n",
    "            # OPTIMIZED parameters\n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=3000,  # INCREASED\n",
    "                learning_rate=0.02,  # DECREASED for better convergence\n",
    "                max_depth=12,  # INCREASED\n",
    "                num_leaves=200,  # INCREASED\n",
    "                subsample=0.80,  # Slightly reduced\n",
    "                colsample_bytree=0.80,  # Slightly reduced\n",
    "                min_child_samples=25,  # Slightly reduced\n",
    "                reg_alpha=0.15,  # INCREASED regularization\n",
    "                reg_lambda=0.15,  # INCREASED regularization\n",
    "                min_split_gain=0.01,  # NEW: Minimum gain to split\n",
    "                random_state=42 + fold,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1,\n",
    "                extra_trees=True  # NEW: Extra randomization\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(150), log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            y_val_pred = model.predict(X_val)\n",
    "            rmse = np.sqrt(np.mean((y_val - y_val_pred) ** 2))\n",
    "            cv_scores.append(rmse)\n",
    "            print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        print(f\"\\n✓ Average CV RMSE: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "        \n",
    "        # Train final model on all data\n",
    "        print(\"\\nTraining final model on full dataset...\")\n",
    "        self.model = lgb.LGBMRegressor(\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.02,\n",
    "            max_depth=12,\n",
    "            num_leaves=200,\n",
    "            subsample=0.80,\n",
    "            colsample_bytree=0.80,\n",
    "            min_child_samples=25,\n",
    "            reg_alpha=0.15,\n",
    "            reg_lambda=0.15,\n",
    "            min_split_gain=0.01,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "            extra_trees=True\n",
    "        )\n",
    "        \n",
    "        # Use 15% validation for early stopping\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.15, random_state=42\n",
    "        )\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[early_stopping(150), log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Final model trained!\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Prediction with dimension check\"\"\"\n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        \n",
    "        if hasattr(self, 'expected_feature_count') and X_test.shape[1] != self.expected_feature_count:\n",
    "            X_test = self.align_features(X_test)\n",
    "        \n",
    "        predictions = self.model.predict(X_test)\n",
    "        \n",
    "        # Smart clipping based on training data percentiles\n",
    "        predictions = np.maximum(predictions, 0.1)\n",
    "        predictions = np.minimum(predictions, 1000)  # INCREASED ceiling\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TRAIN_PATH = r'68e8d1d70b66d_student_resource/student_resource/dataset/train.csv'\n",
    "TEST_PATH = r'68e8d1d70b66d_student_resource/student_resource/dataset/test.csv'\n",
    "SUBMISSION_DIR = r'68e8d1d70b66d_student_resource/student_resource/dataset'\n",
    "\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = MaxAccuracyFeatureEngineer()\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_csv(TRAIN_PATH, on_bad_lines='skip', encoding='utf-8')\n",
    "\n",
    "print(f\"Training set: {len(df_train):,} records\")\n",
    "\n",
    "# Remove outliers\n",
    "print(\"\\nRemoving outliers...\")\n",
    "price_q1 = df_train['price'].quantile(0.005)\n",
    "price_q99 = df_train['price'].quantile(0.995)\n",
    "\n",
    "print(f\"  Price range before: ${df_train['price'].min():.2f} - ${df_train['price'].max():.2f}\")\n",
    "df_train = df_train[(df_train['price'] >= price_q1) & \n",
    "                   (df_train['price'] <= price_q99)]\n",
    "print(f\"  Price range after: ${df_train['price'].min():.2f} - ${df_train['price'].max():.2f}\")\n",
    "print(f\"  Samples after outlier removal: {len(df_train):,}\")\n",
    "\n",
    "# Log transform target\n",
    "print(\"\\nApplying log transform to target...\")\n",
    "y_train_original = df_train['price'].values.astype(np.float32)\n",
    "y_train_log = np.log1p(y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreparing training features...\")\n",
    "X_train, df_train = engineer.prepare_full_features(df_train, is_training=True)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"  - TF-IDF features: {engineer.tfidf.get_feature_names_out().shape[0]}\")\n",
    "print(f\"  - Numeric features: {len(engineer.feature_columns)}\")\n",
    "print(f\"  - Total features: {X_train.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Cross Validation\n",
    "engineer.train_model(X_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading test data...\")\n",
    "df_test = pd.read_csv(TEST_PATH, on_bad_lines='skip', encoding='utf-8')\n",
    "print(f\"Test set: {len(df_test):,} records\")\n",
    "\n",
    "print(\"Preparing test features...\")\n",
    "X_test, df_test = engineer.prepare_full_features(df_test, is_training=False)\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Make predictions and inverse transform\n",
    "predictions_log = engineer.predict(X_test)\n",
    "predictions = np.expm1(predictions_log)\n",
    "\n",
    "# Post-processing\n",
    "print(\"\\nApplying post-processing...\")\n",
    "for idx, row in df_test.iterrows():\n",
    "    category = row.get('category', 'other')\n",
    "    \n",
    "    if category in ['laptop', 'phone', 'tablet', 'gpu', 'cpu']:\n",
    "        if predictions[idx] < 50:\n",
    "            predictions[idx] *= 1.2\n",
    "    \n",
    "    elif category in ['food', 'beauty', 'cooking']:\n",
    "        if predictions[idx] > 100:\n",
    "            predictions[idx] *= 0.85\n",
    "\n",
    "predictions = np.clip(predictions, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis & Submission\n",
    "print(f\"\\nPrediction Analysis:\")\n",
    "print(f\"  Min: ${predictions.min():.2f}\")\n",
    "print(f\"  Max: ${predictions.max():.2f}\")\n",
    "print(f\"  Mean: ${predictions.mean():.2f}\")\n",
    "\n",
    "# Save\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_file = os.path.join(SUBMISSION_DIR, f'outler_both_main{timestamp}.csv')\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'], \n",
    "    'price': predictions\n",
    "})\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ SUBMISSION SAVED: {submission_file}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(submission.head())\n",
    "\n",
    "# Feature Importance Plot\n",
    "if hasattr(engineer.model, 'feature_importances_'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    lgb.plot_importance(engineer.model, max_num_features=20, importance_type='split')\n",
    "    plt.title(\"Top 20 Features by Split Importance\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
